1.
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Sample dataset (replace with actual CSV if available)
data = {'Pregnancies':[6,1,8,1,0,5,3,10],
        'Glucose':[148,85,183,89,137,116,78,115],
        'BloodPressure':[72,66,64,66,40,74,50,70],
        'BMI':[33.6,26.6,23.3,28.1,43.1,25.6,31.0,35.3],
        'Age':[50,31,32,21,33,30,26,29],
        'Outcome':[1,0,1,0,1,0,1,0]}
df = pd.DataFrame(data)

X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Split & scale
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Find optimal K
best_k, best_acc = 1,0
for k in range(1,6):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    acc = accuracy_score(y_test, knn.predict(X_test))
    if acc > best_acc:
        best_k, best_acc = k, acc

# Train with optimal K
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train, y_train)
new_patient = [[45, 130, 70, 30.0, 40]]  # Example new patient
new_patient_scaled = scaler.transform(new_patient)
print(f"Optimal K: {best_k}, Prediction (1=Diabetic,0=Not): {knn.predict(new_patient_scaled)[0]}")

-------------------
2.

import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules
# Sample dataset (assuming a structure of one-hot encoded data)
data = {
'milk': [1, 1, 1, 0, 0, 1],
'bread': [1, 0, 1, 1, 1, 0],
'butter': [0, 1, 1, 1, 0, 1],
'beer': [1, 0, 0, 1, 1, 0]
}
df = pd.DataFrame(data)
# Apply Apriori algorithm with minimum support of 0.25
frequent_itemsets = apriori(df, min_support=0.25, use_colnames=True)
# Display results
print("Frequent Itemsets:")
print(frequent_itemsets)
# Generate association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
print("\nAssociation Rules:")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

--------------------------------------------------------------------------------------------
                          WITH CSV

1.
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# ---------------------------------------------
# Read CSV
# ---------------------------------------------
df = pd.read_csv("diabetes.csv")   # your dataset file

# Features and Target
X = df.drop("Outcome", axis=1)
y = df["Outcome"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Scaling data (KNN works better with scaling)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ---------------------------------------------
# Finding Best K value
# ---------------------------------------------
best_k = 1
best_accuracy = 0

for k in range(1, 21):  # check K from 1 to 20
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    acc = accuracy_score(y_test, pred)

    if acc > best_accuracy:
        best_accuracy = acc
        best_k = k

print("Best K value =", best_k)
print("Best Accuracy =", best_accuracy)

# ---------------------------------------------
# Final Model with Best K
# ---------------------------------------------
final_model = KNeighborsClassifier(n_neighbors=best_k)
final_model.fit(X_train, y_train)

new_patient = [[2, 120, 70, 20, 85, 28.5, 0.35, 33]]  # example input
new_patient = scaler.transform(new_patient)

prediction = final_model.predict(new_patient)
print("New patient diabetic (1=yes, 0=no):", prediction[0])

--------------------------------------

2.

import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

# ---------------------------------------------
# Read CSV (grocery transactions)
# Format: each row contains items bought
# ---------------------------------------------
df = pd.read_csv("groceries.csv", header=None)

# Convert rows into list of lists
transactions = df.values.tolist()

# Convert to one-hot encoding
te = TransactionEncoder()
te_data = te.fit(transactions).transform(transactions)
df_encoded = pd.DataFrame(te_data, columns=te.columns_)

# ---------------------------------------------
# Apply Apriori
# ---------------------------------------------
frequent_items = apriori(df_encoded, min_support=0.25, use_colnames=True)

# Association Rules
rules = association_rules(frequent_items, metric="lift", min_threshold=1)

print("Frequent Itemsets:")
print(frequent_items)

print("\nAssociation Rules:")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
